{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/4%20-%20Preprocessing%20del%20testo/text_preprocessing_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lIngrrjZR0C-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tokenizzazione\n",
        "La tokenizzazione è il processo di estrazione delle parti costituenti del testo, chiamati per l'appunto **tokens**, in sostanza i tokens sono le parole e i caratteri di punteggiatura che compongono il testo.\n",
        "<br>\n",
        "Vediamo come eseguire la tokenizzazione con nltk, importiamo il modulo"
      ]
    },
    {
      "metadata": {
        "id": "CKp4HdRwP9Il",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mG2uOccZsIg6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Per eseguire la tokenizzazione, dobbiamo scaricare il pacchetto di dati 'punkt'."
      ]
    },
    {
      "metadata": {
        "id": "trbKIsjMRbXy",
        "colab_type": "code",
        "outputId": "7e2d3f90-356a-49ef-ab50-b9bfd8d85ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "PUZubpMOspJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Per eseguire la tokenizzazione possiamo usare la funzione *word_tokenizer(sentence)* di nltk."
      ]
    },
    {
      "metadata": {
        "id": "qbkzAN_4Rkpk",
        "colab_type": "code",
        "outputId": "41e95ff8-b34f-4bc6-d2b4-cc82f66e6397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "text = \"Cos'è questa fretta? Facciamolo un'altra volta, ti va bene?\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Cos'è\", 'questa', 'fretta', '?', 'Facciamolo', \"un'altra\", 'volta', ',', 'ti', 'va', 'bene', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ns0A9_sPtf4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi il risultato è una lista dei tokens, cioè di parole e punteggiatura che costituiscono la frase, ma non avremmo ottenuto lo stesso risultato usando il metodo *split()* ? Vediamolo"
      ]
    },
    {
      "metadata": {
        "id": "Dp_MmD2Ttzrc",
        "colab_type": "code",
        "outputId": "5ff86770-497d-40c4-cac9-63aafeacbb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "text = \"Cos'è questa fretta? Facciamolo un'altra volta, ti va bene?\"\n",
        "text.split()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Cos'è\",\n",
              " 'questa',\n",
              " 'fretta?',\n",
              " 'Facciamolo',\n",
              " \"un'altra\",\n",
              " 'volta,',\n",
              " 'ti',\n",
              " 'va',\n",
              " 'bene?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "nrs0h3YGt7Ko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi il risultato non è lo stesso, il metodo *split()* semplicemente separa le parole in base agli spazi, quindi la punteggiatura non viene isolata.\n",
        "<br>\n",
        "Oltre a questo la tokenizzazione ci dovrebbe permette di dividere parole unite in forma contratta (Cos'è = Cosa+è). Purtroppo la funzione *word_tokenizer* non supporta la lingua italiana, facciamo una prova con l'inglese."
      ]
    },
    {
      "metadata": {
        "id": "mUeC0RIpQAA7",
        "colab_type": "code",
        "outputId": "9bcdacf0-cc8f-4e6d-fc1f-85452a648814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text = \"I'd love to visit U.S.A, but I can't afford it!\" # Mi piacerebbe visitare gli Stati Uniti ma non posso permettermelo\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', \"'d\", 'love', 'to', 'visit', 'U.S.A', ',', 'but', 'I', 'ca', \"n't\", 'afford', 'it', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AoWF_gBJv5FA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi, con la lingua inglese, la funzione *word_tokenizer* è riuscita a separare anche le parole unite in forma contratta, nota anche come è riuscita a comprendere che U.S.A è un'unica parola.\n",
        "<br>\n",
        "Possiamo tokenizzare anche intere frasi piuttosto che singole parole, per farlo dobbiamo usare la funzione *sent_tokenize"
      ]
    },
    {
      "metadata": {
        "id": "AfCmFAbC-7hT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "319fb284-3ee8-4ff1-dbfa-4b38d6d12537"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"I'd love to visit U.S.A, but I can't afford it. Instead this summer I'll visit U.K\" # Mi piacerebbe visitare gli Stati Uniti ma non posso permettermelo. Invece questa estate visiterò il Regno Unito. \n",
        "\n",
        "tokens_sent = sent_tokenize(text)\n",
        "\n",
        "print(tokens_sent)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"I'd love to visit U.S.A, but I can't afford it.\", \"Instead this summer I'll visit U.K\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ava0Sk6yZb6D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stop Words\n",
        "Le stop words sono quelle parole comuni del linguaggio che portano poca o nessuna informazione al discorso, esempi possono essere le congiunzioni o verbi comuni come il verbo essere e il verbo avere.\n",
        "<br>\n",
        "NLTK contiene liste di stop words per diversi linguaggi, per ottenerle dobbiamo prima scaricare il corpus 'stopwords'."
      ]
    },
    {
      "metadata": {
        "id": "MZtHEWhcaEV2",
        "colab_type": "code",
        "outputId": "e976016a-caf7-44b0-92fc-f930f3320465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "YPyGJZn1arEG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Poi possiamo ottenere una lista per una determinata lingua utilizzando la funzione *words* del metodo *stopwords*, indicando al suo interno la lingua per la quale vogliamo avere le stop words."
      ]
    },
    {
      "metadata": {
        "id": "1XdTxFy3Z5lx",
        "colab_type": "code",
        "outputId": "93b0affe-adc1-4ea4-998c-a6f5d057cde0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "en_stopwords = stopwords.words('english')\n",
        "\n",
        "print(\"Stop words totali: %d\" % len(en_stopwords))\n",
        "print(\"Prime 10 stop words: %s\" % en_stopwords[:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stop words totali: 179\n",
            "Prime 10 stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "phitv0LTa8Ih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi per l'inglese abbiamo 179 stop words, vediamo per la lingua italiana."
      ]
    },
    {
      "metadata": {
        "id": "EjNIQLLFafK_",
        "colab_type": "code",
        "outputId": "4702cb0c-0955-4b12-a413-cda569c236e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "it_stopwords = stopwords.words('italian')\n",
        "\n",
        "print(\"Stop words totali: %d\" % len(it_stopwords))\n",
        "print(\"Prime 10 stop words: %s\" % it_stopwords[:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stop words totali: 279\n",
            "Prime 10 stop words: ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LP7X7neAbEL4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Per l'italiano abbiamo 279 parole, possiamo rimuovere le stop words dalla nostra lista di tokens iterando sopra di essi e verificando se il token è presente o meno nella lista di stop words."
      ]
    },
    {
      "metadata": {
        "id": "WHHXpHzWboJe",
        "colab_type": "code",
        "outputId": "3a7c5705-3592-4783-e08c-c4d16a836a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "text = \"Io sono una persona simpatica, solo che non mi piacciono gli esseri umani, preferisco i gatti\"\n",
        "\n",
        "it_stopwords = stopwords.words('italian')\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "tokens_filtered = []\n",
        "\n",
        "for token in tokens:\n",
        "  if(token.lower() not in it_stopwords):\n",
        "    tokens_filtered.append(token)\n",
        "    \n",
        "    \n",
        "print(\"Tokens originali: %s\" % tokens)\n",
        "print(\"Tokens rimasti: %s\" % tokens_filtered)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens originali: ['Io', 'sono', 'una', 'persona', 'simpatica', ',', 'solo', 'che', 'non', 'mi', 'piacciono', 'gli', 'esseri', 'umani', ',', 'preferisco', 'i', 'gatti']\n",
            "Tokens rimasti: ['persona', 'simpatica', ',', 'solo', 'piacciono', 'esseri', 'umani', ',', 'preferisco', 'gatti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hONBoj60cldY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi abbiamo convertito la parola in minuscolo per assicurarsi il matching anche delle parole a inizio frase."
      ]
    },
    {
      "metadata": {
        "id": "xiOr2eubR3MF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Lo stemming è un processo che consiste nel ridurre le parole di un testo nella loro forma base, chiamata appunto **stem**.\n",
        "<br><br>\n",
        "NLTK ci mette a disposizione due oggetti per eseguire lo stemming, il PorterStemmer e lo SnowballStemmer."
      ]
    },
    {
      "metadata": {
        "id": "JzMDMKHC4L5q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Porter Stemmer\n",
        "Il Poter Stemmer è un'algoritmo di stemming che applica una serie di regole in 5 steps per rimuovere il suffisso dalla frase.\n",
        "<br>\n",
        "Importiamo la classe *PorterStemmer* da nltk e testiamola su una frase, questo algoritmo funziona solo con la lingua inglese."
      ]
    },
    {
      "metadata": {
        "id": "LVEeEfNUQFzp",
        "colab_type": "code",
        "outputId": "9cfeb4c7-978c-4022-e533-7ca7c7d4d90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# lo stemmer prende in input la singola parola\n",
        "# quindi passiamo un token per volta\n",
        "# utilizzando un ciclo for\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tSTEM\n",
            "My\t\tMy\n",
            "cats\t\tcat\n",
            "ate\t\tate\n",
            "some\t\tsome\n",
            "mice\t\tmice\n",
            "while\t\twhile\n",
            "I\t\tI\n",
            "was\t\twa\n",
            "playing\t\tplay\n",
            "King\t\tking\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\triddl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rc2Yw_kf7VYq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Snowball Stemmer\n",
        "Snowball è un'altro algoritmo di stemming che apporta alcune migliorie al Porter Stemmer e a differenza di questo supporta diverse lingue con NLTK, incluso l'italiano.\n",
        "<br>\n",
        "Possiamo passare la lingua come parametro dello stemmer."
      ]
    },
    {
      "metadata": {
        "id": "JHd6iP4ESa5w",
        "colab_type": "code",
        "outputId": "8688c0ca-fb35-4322-c50c-33e79b742b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tSTEM\n",
            "My\t\tmy\n",
            "cats\t\tcat\n",
            "ate\t\tate\n",
            "some\t\tsome\n",
            "mice\t\tmice\n",
            "while\t\twhile\n",
            "I\t\ti\n",
            "was\t\twas\n",
            "playing\t\tplay\n",
            "King\t\tking\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\triddl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8MeHyejFTuea",
        "colab_type": "code",
        "outputId": "34d86eb4-ec27-434e-b0e4-9a1a1e2c7341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "text = \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "stemmer = SnowballStemmer(\"italian\")\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tSTEM\n",
            "I\t\ti\n",
            "miei\t\tmie\n",
            "gatti\t\tgatt\n",
            "hanno\t\thann\n",
            "mangiato\t\tmang\n",
            "alcuni\t\talcun\n",
            "topi\t\ttop\n",
            "mentre\t\tmentr\n",
            "io\t\tio\n",
            "stavo\t\tstav\n",
            "giocavo\t\tgioc\n",
            "a\t\ta\n",
            "il\t\til\n",
            "signore\t\tsignor\n",
            "degli\t\tdegl\n",
            "enigmi\t\tenigm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "En69Ke2j8Djh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ovviamente la precisione dell'algoritmo è maggiore per la lingua inglese che per la lingua italiana."
      ]
    },
    {
      "metadata": {
        "id": "C5PFDdUHPzQk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lancaster Stemmer\n",
        "Il Lancaster è un'altro algoritmo di stemming che applica regole più aggressive per la riduzione delle parole, anche eccessivamente aggressive, facciamo un esempio."
      ]
    },
    {
      "metadata": {
        "id": "VWA20mfXP1ie",
        "colab_type": "code",
        "outputId": "0b0d4834-88a2-433e-c958-c67d6e42051a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "stemmer=LancasterStemmer()\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tSTEM\n",
            "My\t\tmy\n",
            "cats\t\tcat\n",
            "ate\t\tat\n",
            "some\t\tsom\n",
            "mice\t\tmic\n",
            "while\t\twhil\n",
            "I\t\ti\n",
            "was\t\twas\n",
            "playing\t\tplay\n",
            "King\t\tking\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\triddl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uyOr_FaxQaVw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi usando il Lancaster alcune parole sono state tagliate. Nella pratica possiamo adottare tranquillamente lo Snowball, il quale dovrebbe portare a risultati più stabili e precisi."
      ]
    },
    {
      "metadata": {
        "id": "SUrgr8xbErxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lemmatizatione\n",
        "La lemmatizzazione ci permette di ridurre una parola dalla sua forma flessa alla sua forma canonica, detta appunto **lemma**, il quale a differenza dello stem è una parola reale.\n",
        "Possiamo eseguire la lemmatizzazione con nltk usando il *WordNetLemmatizer*."
      ]
    },
    {
      "metadata": {
        "id": "iZ_qaI2yMp37",
        "colab_type": "code",
        "outputId": "70b06b19-449e-4b10-b691-12c22e94f9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "u8siCFEnEqmB",
        "colab_type": "code",
        "outputId": "92cddd78-68fd-4490-a569-f13882513d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "text = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"TOKEN\\t\\tLEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, lemmatizer.lemmatize(token)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tLEM\n",
            "My\t\tMy\n",
            "cats\t\tcat\n",
            "ate\t\tate\n",
            "some\t\tsome\n",
            "mice\t\tmouse\n",
            "while\t\twhile\n",
            "I\t\tI\n",
            "was\t\twa\n",
            "playing\t\tplaying\n",
            "King\t\tKing\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\tRiddles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UFXy2CVOXD82",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi a lemmatizzato perfettamente i nomi (mice -> mouse), però non ha fatto altrettanto con i verbi, perchè ? Perché dobbiamo anche specificare il tipo di parola utilizzando il parametro pos, se non lo facciamo crederà che si tratti di un nome.\n"
      ]
    },
    {
      "metadata": {
        "id": "i3wH36NLMfhB",
        "colab_type": "code",
        "outputId": "220dfa2a-3d7f-4eec-e186-17391408694f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokens = [(\"My\",\"n\"),(\"cats\",\"n\"),(\"ate\",\"v\"), (\"some\",\"n\"), (\"mice\",\"n\"), (\"while\",\"r\"), (\"I\",\"n\"), (\"was\",\"v\"), (\"playing\",\"v\"), (\"King\",\"v\"), (\"of\",\"n\"),(\"the\",\"n\"),(\"Riddles\",\"n\")]\n",
        "\n",
        "print(\"TOKEN\\t\\tLEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token[0], lemmatizer.lemmatize(token[0], pos=token[1])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tLEM\n",
            "My\t\tMy\n",
            "cats\t\tcat\n",
            "ate\t\teat\n",
            "some\t\tsome\n",
            "mice\t\tmouse\n",
            "while\t\twhile\n",
            "I\t\tI\n",
            "was\t\tbe\n",
            "playing\t\tplay\n",
            "King\t\tKing\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\tRiddles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1IaEPSmCY0n6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pos sta per part-of-speech, cioè parte del discorso, è possibile automatizzare l'identificazione della parte del discorso in modo da non doverli inserire manualmente, nella prossima sezione vedremo come."
      ]
    }
  ]
}