{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/09%20-%20Word%20embedding%20e%20Word2Vec/word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNb6_Q5_f2vE",
        "colab_type": "text"
      },
      "source": [
        "## Word embedding\n",
        "Il **Word embedding** è un modello che ci permette di generare una serie di vettori (embedding vectors) ognuno dei quali quantifica una caratteristica delle parole. In questo notebook creeremo una rete neurale artificiale per classsificare recensioni come positive o negative usando il Word Embedding per codificare le recensioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cSDivbYhDhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85816c17-8127-416d-d852-a2fc5bd260bd"
      },
      "source": [
        "!pip install numpy==1.16.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyB8q8Aw1tqp",
        "colab_type": "text"
      },
      "source": [
        "## Prepariamo i dati\n",
        "In precedenza abbiamo caricato e preprocessato manualmente l'IMDB Movies Review Dataset, in questo caso utilizziamo direttamente il dataset già preprocessato che ci mette a disposizione Keras.\n",
        "<br>**ATTENZIONE**<br>\n",
        "Se caricando il dataset ottieni questo errore:<br>\n",
        "*ValueError: Object arrays cannot be loaded when allow_pickle=False*\n",
        "<br>\n",
        "questo è casuato da un bug nell'ultima versione di keras, per correggerlo esegui il downgrade di numpy usando la cella di codice qui sotto e riavvia il kernel (su Colaboratory seleziona Runtime dalla barra dei comandi e clicca su Restart Runtime)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP00t5BP3PNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy==1.16.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVFYLaPrf054",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aed8a090-3a31-4f2f-9f8e-0eea866111dc"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb \n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "print(\"Numero di esempi nel train set: %d\" % len(X_train))\n",
        "print(\"Numero di esempi nel test set: %d\" % len(X_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero di esempi nel train set: 25000\n",
            "Numero di esempi nel test set: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jvHkFhC3T30",
        "colab_type": "text"
      },
      "source": [
        "Ogni riga della lista con le features corrisponde ad una frase, ogni colonna contiene l'indice di una parola all'interno del vocabolario dell'intero corpus di testo. Il vettore con il target contiene un unico valore che può essere 0 per una recensione negativa o 1 per una recensione positiva.<br> \n",
        "Definiamo una funzione che ci permette di ricostruire la frase partendo dagli indici, per farlo abbiamo bisogno del vocabolario che mappa le parole agli indici, possiamo ottenerlo con il metodo *.get_word_index()*.\n",
        "<br>\n",
        "**NOTA BENE**\n",
        "<br>\n",
        "Gli indici delle parole hanno un'offset di 3, quindi per ottenere l'indice corretto per il vocabolario dovremo sottrarre 3 all'indice della parola contenuto nella frase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lddFN1OX52C3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f21ae533-fb43-478f-84ef-f5644e55193a"
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "index_word = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def vec_to_text(x):\n",
        "  \n",
        "  text = [index_word.get(i-3,'?') for i in x]\n",
        "  return \" \".join(text)\n",
        "  \n",
        "vec_to_text(X_test[0])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite ? so all you madison fans give this a miss\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCvLErNe8aSk",
        "colab_type": "text"
      },
      "source": [
        "Ovviamente le recensioni avranno lunghezza differente, calcoliamo la lunghezza della più lunga e della più corta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpphdUrrg-gD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fcfca3e4-55af-493f-ce75-e117bf87b61e"
      },
      "source": [
        "longest_review = max(X_train,key=len)\n",
        "shortest_review = min(X_train,key=len)\n",
        "\n",
        "print(\"La review più lunga ha %d parole\" % len(longest_review))\n",
        "print(\"La review più corta ha %d parole\" % len(shortest_review))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La review più lunga ha 2494 parole\n",
            "La review più corta ha 11 parole\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02dun4qF3bjU",
        "colab_type": "text"
      },
      "source": [
        "Per rendere le features un buon input per il nostro modello dobbiamo fare in modo che ogni frase abbia la stessa lunghezza, per farlo possiamo usare la funzione *pad_sequences(text)* di keras, che riduce tutte le frasi ad una lunghezza prefissata troncando quelle troppo lunghe e aggiungendo degli zeri a quelle troppo brevi. Usiamo una lunghezza comune di 50 parole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NybngBzhRGQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c652169c-e934-47d6-bc5b-1a22297cab8d"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 50\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen = maxlen)\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-M5pPcD1wB2",
        "colab_type": "text"
      },
      "source": [
        "## Creiamo il modello\n",
        "Possiamo utilizzare l'embedding come se fosse uno strato della nostra rete neurale che verrà anch'esso ottimizzato durante la fase di addestramento. Creiamo la rete aggiungendo al primo strato uno strato di embedding, dopodichè aggiungiamo un'altro strato che utilizza la classe Flatten() di keras per convertire la matrice che contiene la rappresentazione vettoriale di una frase in un vettore, unendo tutte le righe una dietro l'altra. Infine aggiungiamo uno strato di output per eseguire la classificazione binaria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiflX3qnhUIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "38b4e184-f03b-4061-aed0-274704469fc5"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 50, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 50, 50)            500000    \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2500)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 2501      \n",
            "=================================================================\n",
            "Total params: 502,501\n",
            "Trainable params: 502,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueLGoyMK4O7A",
        "colab_type": "text"
      },
      "source": [
        "Compiliamo il modello e avviamo l'addestramento per 10 epoche."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD6yja5whYuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "91cfc754-6c2c-41d5-e8e2-27b18bcc945e"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=10)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 1s 61us/step - loss: 0.6890 - acc: 0.5621 - val_loss: 0.6786 - val_acc: 0.6538\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 1s 35us/step - loss: 0.6346 - acc: 0.7770 - val_loss: 0.6023 - val_acc: 0.7410\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 1s 35us/step - loss: 0.5101 - acc: 0.8193 - val_loss: 0.4969 - val_acc: 0.7790\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 1s 37us/step - loss: 0.3932 - acc: 0.8570 - val_loss: 0.4415 - val_acc: 0.7982\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 1s 36us/step - loss: 0.3183 - acc: 0.8892 - val_loss: 0.4151 - val_acc: 0.8054\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 1s 35us/step - loss: 0.2630 - acc: 0.9158 - val_loss: 0.4053 - val_acc: 0.8076\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 1s 36us/step - loss: 0.2191 - acc: 0.9364 - val_loss: 0.4021 - val_acc: 0.8112\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 1s 35us/step - loss: 0.1822 - acc: 0.9553 - val_loss: 0.4048 - val_acc: 0.8116\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 1s 35us/step - loss: 0.1494 - acc: 0.9709 - val_loss: 0.4085 - val_acc: 0.8120\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 1s 35us/step - loss: 0.1217 - acc: 0.9814 - val_loss: 0.4146 - val_acc: 0.8120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa4a0abc0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JXeWNcxhfrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "61b5b9c2-936c-47c5-b4c5-446ca0b37cdb"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 31us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.40605927324295044, 0.815]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1LAyqsu1zs4",
        "colab_type": "text"
      },
      "source": [
        "## Otteniamo gli embedding\n",
        "Se volessimo conoscere gli embedding che il modello genera per una determinata frase possiamo farlo creando un nuovo modello che da in output l'output dell'embedding che abbiamo addestrato. Keras ci da la possibilità di accedere ai singoli strati di un modello utilizzato l'attributo *.layers*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTPAimqL5NZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d87a8aac-95af-4360-d5e5-af4e4f5ea661"
      },
      "source": [
        "model.layers[0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7fa4abdc33c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meavLoRp5Qg0",
        "colab_type": "text"
      },
      "source": [
        "Utilizzando le API Funzionali di Keras (ne parleremo più avanti) possiamo creare un modello che prende in input lo stesso input del modello addestrato e da in output l'output dell'embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqGhALMG06Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "embedding_model = Model(inputs=model.input, outputs=model.layers[0].output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZpItTpc5fIt",
        "colab_type": "text"
      },
      "source": [
        "Utilizzando il metodo *.predict(x)* otterremo una matrice con la rappresentazione vettoriale di ogni parola della frase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTh6OMit1VJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "6c3bb837-106b-473b-a5eb-7d8cbf9fbb2e"
      },
      "source": [
        "x = np.array([X_test[0]])\n",
        "\n",
        "embedding_model.predict(x)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.30276826,  0.111716  ,  0.17554781, ...,  0.2054588 ,\n",
              "         -0.23213504,  0.12865287],\n",
              "        [-0.10857813,  0.00208598, -0.11951214, ..., -0.20470296,\n",
              "         -0.12110917, -0.09332115],\n",
              "        [-0.05576379,  0.0109272 , -0.00774001, ...,  0.02854038,\n",
              "         -0.02343158,  0.02618085],\n",
              "        ...,\n",
              "        [ 0.08926877, -0.00070353, -0.05446988, ...,  0.07803367,\n",
              "         -0.0772426 , -0.10260648],\n",
              "        [-0.0244258 , -0.0769368 , -0.12284474, ..., -0.02184814,\n",
              "          0.03313072, -0.04578592],\n",
              "        [-0.07755034,  0.29535306, -0.04420415, ..., -0.05049261,\n",
              "         -0.04422899, -0.03766544]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    }
  ]
}