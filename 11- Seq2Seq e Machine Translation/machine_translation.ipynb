{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_translation_char.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/10%20-%20Seq2Seq%20e%20Machine%20Translation/machine_translation_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vgs3guLrm2jR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Architetture Sequence 2 Sequence per la traduzione automatica\n",
        "In questo notebook creeremo un'architettura **Sequence 2 Sequence** di rete neurale artificiale in grado di tradurre del testo dall'italiano all'inglese, per farlo avremo bisogno di un corpus di testo contenente frasi di esempio in entrambe le lingue, fortunatamente il software di flashcard Anki ci mette a disposizione tali corpus per molteplici lingue, scarichiamo quello per inglese-italiano da [questo link](http://www.manythings.org/anki/). Se usi Google colab o hai wget installato, scarica pure il file zip eseguendo la cella di codice qui sotto."
      ]
    },
    {
      "metadata": {
        "id": "UYis2kuN38JE",
        "colab_type": "code",
        "outputId": "886b7b10-2a83-4712-d59b-c74171b30c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/ita-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-02 14:17:44--  http://www.manythings.org/anki/ita-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6dc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3981147 (3.8M) [application/zip]\n",
            "Saving to: ‘ita-eng.zip’\n",
            "\n",
            "ita-eng.zip         100%[===================>]   3.80M  3.13MB/s    in 1.2s    \n",
            "\n",
            "2019-05-02 14:17:46 (3.13 MB/s) - ‘ita-eng.zip’ saved [3981147/3981147]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "opMRzXz4nzVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ed estrai lo zip"
      ]
    },
    {
      "metadata": {
        "id": "AhiiXObg4Ai7",
        "colab_type": "code",
        "outputId": "eb5dfff5-f076-47d3-f254-fa6a4e7c86d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip ita-eng.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ita-eng.zip\n",
            "  inflating: ita.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4_kvjb7-n2cA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso abbiamo il file ita.txt contenente coppie di frasi in inglese e italiano su ogni riga, separate da un tab. Dividiamo le frasi in base alla lingua in due liste separate e salviamo ogni carattere all'interno di un set, mentre scorriamo il file processiamo anche le frasi, rimuovendo la punteggiatura e convertendo tutto in minuscolo."
      ]
    },
    {
      "metadata": {
        "id": "yanlWnxW4B1t",
        "colab_type": "code",
        "outputId": "e6cb9ba8-3bc9-4953-b11a-bc1b021aeb8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "en_chars = set({})\n",
        "it_chars = set({})\n",
        "\n",
        "it_sents = []\n",
        "en_sents = []\n",
        "\n",
        "with open(\"ita.txt\") as en_it_sents:\n",
        "  lines = en_it_sents.read().split(\"\\n\")\n",
        "  \n",
        "  num_samples = 100000\n",
        "  \n",
        "  for line in lines[:min(num_samples, len(lines)-1)]:\n",
        "    \n",
        "    line = re.sub(r'[^\\w\\s]','',line)\n",
        "    line = line.lower()\n",
        "    \n",
        "    en_sent, it_sent = line.split(\"\\t\")\n",
        "    en_sent = \"\\t\"+en_sent+\"\\n\"\n",
        "    \n",
        "    en_sents.append(en_sent)\n",
        "    it_sents.append(it_sent)\n",
        "    \n",
        "    for char in en_sent:\n",
        "      en_chars.add(char)\n",
        "      \n",
        "    for char in it_sent:\n",
        "      it_chars.add(char)\n",
        "      \n",
        "print(\"Numero di frasi di esempio %d\" % num_samples)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero di frasi di esempio 100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sX6Kzdnqp_MS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Convertiamo i set di caratteri in liste ordinate e contiamo il numer do caratteri contenuti in ogni lista."
      ]
    },
    {
      "metadata": {
        "id": "jq33OblVqBxy",
        "colab_type": "code",
        "outputId": "ce5c0292-523d-40be-eed9-cd1a838abc06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "it_chars = sorted(list(it_chars))\n",
        "en_chars = sorted(list(en_chars))\n",
        "\n",
        "num_encoder_tokens = len(it_chars)\n",
        "num_decoder_tokens = len(en_chars)\n",
        "\n",
        "print('Numbero di token di input:', num_encoder_tokens)\n",
        "print('Number di token di output:', num_decoder_tokens)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numbero di token di input: 46\n",
            "Number di token di output: 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SCWhad-3qI_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Contiamo la lungezza massima di una frase per entrambe le lingue."
      ]
    },
    {
      "metadata": {
        "id": "oEld6kbH6ts5",
        "colab_type": "code",
        "outputId": "35b6c866-d97a-48c1-85bd-ff9c78d60a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "max_encoder_seq_length = max([len(sent) for sent in it_sents])\n",
        "max_decoder_seq_length = max([len(sent) for sent in en_sents])\n",
        "\n",
        "print(\"Lunghezza massima della sequenza per l'input:\", max_encoder_seq_length)\n",
        "print(\"Lunghezza massima della sequenza per l'output:\", max_decoder_seq_length)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lunghezza massima della sequenza per l'input: 100\n",
            "Lunghezza massima della sequenza per l'output: 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZoQNtRnGqsmu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso dobbiamo eseguire il one hot encoding di ogni frase a livello di carattere, per farlo ci conviene creare il dizionario che ci permette di accedere velocemente all'indice partendo dal carattere."
      ]
    },
    {
      "metadata": {
        "id": "dv89Vjd37s-s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "it_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(it_chars)])\n",
        "en_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(en_chars)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IinfEQC1q_cB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Eseguiamo il one hot encoding per creare:\n",
        "- **input dell'encodere**: le frasi italiane.\n",
        "- **input del decoder**: le frasi in inglese.\n",
        "- **output del decoder**: le frasi in inglese shiftate di un carattere."
      ]
    },
    {
      "metadata": {
        "id": "25Z6nmY67xkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# inizializziamo gli array vuoti\n",
        "\n",
        "encoder_input_data = np.zeros((len(it_sents), max_encoder_seq_length, num_encoder_tokens))\n",
        "decoder_input_data = np.zeros((len(it_sents), max_decoder_seq_length, num_decoder_tokens))\n",
        "decoder_target_data = np.zeros((len(it_sents), max_decoder_seq_length, num_decoder_tokens))\n",
        "\n",
        "# iteriamo simultaneamente su frasi in italiano e inglese\n",
        "\n",
        "for i, (it_sent, en_sent) in enumerate(zip(it_sents, en_sents)):\n",
        "    for t, char in enumerate(it_sent):\n",
        "        # assegnamo un 1 all'indice di ogni carattere contenuto nella frase\n",
        "        encoder_input_data[i, t, it_token_index[char]] = 1.\n",
        "    for t, char in enumerate(en_sent):\n",
        "        # assegnamo un 1 all'indice di ogni carattere contenuto nella frase\n",
        "        decoder_input_data[i, t, en_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # shiftiamo di uno il target\n",
        "            decoder_target_data[i, t - 1, en_token_index[char]] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L97TUB63sbqk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I dati per l'addestramento sono pronti, possiamo passare alla creazione del modello."
      ]
    },
    {
      "metadata": {
        "id": "BQQKb2WtpxwV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creazione del Modello\n",
        "Il modello che andremo a creare è molto complesso e alcuni dei suoi strati devono accettare input da strati differenti (il decoder prende lo stato dall'encoder e come input le frasi in inglese), dobbiamo utilizzare le [API Funzionali di Keras](https://keras.io/getting-started/functional-api-guide/). \n",
        "<br>\n",
        "Cominciamo con l'**encoder**:\n",
        "- Usiamo la classe **Input** per definire l'input dell'encoder.\n",
        "- Creiamo lo strato ricorrente (LSTM) dell'encoder, che dovrà restituire lo stato (return_state=True)\n",
        "- Usiamo l'encoder, scartiamo l'ouput (primo array ritornato) e teniamo solo lo stato (secondo e terzo array)\n"
      ]
    },
    {
      "metadata": {
        "id": "wsvK1SOL73xh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(256, return_state=True)\n",
        "_, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QsFyK7iOwC0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso creiamo il **decoder**:\n",
        "- Usiamo la classe **Input** per definire l'input dell decoder.\n",
        "- Creiamo lo strato ricorrente (LSTM) dell'encoder, che dovrà usare lo stato dell'encoder (initial_state=encoder_states)\n",
        "- Usiamo l'encoder, scartiamo l'ouput (primo array ritornato) e teniamo solo lo stato (secondo e terzo array).\n",
        "- Creiamo uno strato di output che utilizzerà la funzine di attivazione softmax per eseguire una classificazione multiclasse.\n"
      ]
    },
    {
      "metadata": {
        "id": "hvzQXCZOs1r0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fRqMX6ZHx-RR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Usiamo la classe **Model** di keras per creare il modello, passando gli strati di input (sia quello dell'encoder che quello del decoder) all'interno di una lista e lo strato di output."
      ]
    },
    {
      "metadata": {
        "id": "4XVB7mpCtOKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-5yPt0syNqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello, usiamo come algoritmo di ottimizzazione 'rmsprop', il quale dovrebbe portare risultati migliori quando si lavora con modelli sequenziali."
      ]
    },
    {
      "metadata": {
        "id": "o25hndZK8VpL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q4w7G7slyXeH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ed avviamo l'addestramento, qui dobbiamo passare sempre i dati di input, sia per l'encoder che per il decoder, all'interno di una lista e i dati di output.\n",
        "<br><br>\n",
        "**NOTA BENE** \n",
        "<br>\n",
        "Se non hai una GPU che supporta la tecnologia CUDA e non vuoi usare Google Colaboratory, ti consiglio di importare il modello pre-addestrato eseguendo il codice nella cella poco più in basso, altrimenti l'addestramento potrebbe richiedere anche giorni e mettere sotto forte stress il tuo pc.\n"
      ]
    },
    {
      "metadata": {
        "id": "HwmryR4V8dIg",
        "colab_type": "code",
        "outputId": "cebc6286-8616-4816-911f-916bf6c23763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=256,\n",
        "          epochs=100,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c4a01bcd1276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           validation_split=0.2)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Hx1RbeIoLtuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se stai usando una GPU che supporta la tecnologia CUDA, sul tuo computer o con Google Colaboratory, l'addestramento per 500 epoche dovrebbe richiedere un paio di ore, se non vuoi aspettare puoi ridurre il numero di epoche a non meno di 100 oppure importare il modello che ho già addestrato eseguendo il codice qui sotto."
      ]
    },
    {
      "metadata": {
        "id": "KzzgrojUK7po",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_model_from_url(url):\n",
        "\n",
        "  from urllib.request import urlretrieve\n",
        "  from keras.models import load_model\n",
        "\n",
        "  model_file = url[url.rfind(\"/\")+1:]\n",
        "  model_path = url\n",
        "\n",
        "  urlretrieve(model_path, model_file)\n",
        "\n",
        "  model = load_model(model_file)\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = load_model_from_url(\"https://github.com/ProfAI/nlp00/raw/master/10%20-%20Seq2Seq%20e%20Machine%20Translation/model/translate_500.h5\")\n",
        "#model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WziMjgjcp2mb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Usare il modello per tradurre del testo\n",
        "Per calcolare l'output della rete, cioè la frase tradotta, dobbiamo dividere il modello addestrato in due modelli, uno per l'encoder"
      ]
    },
    {
      "metadata": {
        "id": "3iwBAUUOTWao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHa_C6d9sNj6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ed uno per il decoder"
      ]
    },
    {
      "metadata": {
        "id": "kSzBCb1F8fmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XvbxjjzxhJXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Se abbiamo importato il modello pre-addestrato, allora importiamo anche quelli per encoder e decoder**"
      ]
    },
    {
      "metadata": {
        "id": "UivnLW7ihQWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a125e076-2087-4c89-fb4e-4ec5e7d8d9e9"
      },
      "cell_type": "code",
      "source": [
        "encoder_model = load_model_from_url(\"https://github.com/ProfAI/nlp00/raw/master/10%20-%20Seq2Seq%20e%20Machine%20Translation/model/translate_encoder_100.h5\")\n",
        "decoder_model = load_model_from_url(\"https://github.com/ProfAI/nlp00/raw/master/10%20-%20Seq2Seq%20e%20Machine%20Translation/model/translate_decoder_100.h5\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CRkgYUJuSOhB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Definiamo una funzione che ci permette di eseguire il one hot encoding dell'input."
      ]
    },
    {
      "metadata": {
        "id": "Q8_ONvW4-aJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  \n",
        "  text_encoded = np.zeros((max_encoder_seq_length, num_encoder_tokens))\n",
        "\n",
        "  for c, char in enumerate(text):\n",
        "        text_encoded[c, it_token_index[char]] = 1.\n",
        "      \n",
        "  return np.array([text_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQQoRyPlNNuV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_index_token = dict(\n",
        "    (i, char) for char, i in en_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i88T-B7cQ_0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso gli step per eseguire la predizione sono i seguenti:\n",
        "1. Codifichiamo la frase da tradurre utilizzando il one hot encoding.\n",
        "2. Utilizziamo l'encoder per ottenere lo stato iniziale\n",
        "3. Creiamo una sequenza da dare come input iniziale al decoder, contente soltanto il carattere di inizio sequenza /t.\n",
        "4. Utilizziamo il decoder per ottenere il carattere successivo della sequenza e i nuovi stati.\n",
        "5. Aggiungiamo il carattere alla sequenza.\n",
        "6. Ripetiamo gli step 4 e 5 fino a quando non incontriamo il carattere di fine sequenza /n o fino a quando la lunghezza della sequenza non è maggiore alla massima consetita. "
      ]
    },
    {
      "metadata": {
        "id": "0M1nsXAi9Nky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "  \n",
        "    text = encode(text)\n",
        "  \n",
        "    # Otteniamo lo stato inziale usando l'encoder\n",
        "    states_value = encoder_model.predict(text)\n",
        "\n",
        "    # Creiamo un'array vuoto per l'input del decoder\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Inseriamo in prima posizione il carattere di inizio sequenza\n",
        "    target_seq[0, 0, en_token_index['\\t']] = 1.\n",
        "\n",
        "    stop = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop:\n",
        "        \n",
        "        # Usiamo il decoder per predire il seguente carattere della sequenza\n",
        "        # e i nuovi stati\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Estraiamo il carattere usando l'indice predetto\n",
        "        # e aggiungiamolo alla sequenza\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = en_index_token[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Se abbiamo incontrato il carattere di fine sequenza\n",
        "        # o se la lunghezza della sequenza è maggiore della lunghezza massima\n",
        "        # interrompiamo\n",
        "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop = True\n",
        "\n",
        "        # Aggiorniamo l'input del decoder\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Aggiorniamo gli stati\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zy5ihf6TSYxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "E' tutto pronto ! Proviamo ad utilizzare il nostro traduttore."
      ]
    },
    {
      "metadata": {
        "id": "X7bru3jwSzQ7",
        "colab_type": "code",
        "outputId": "56805bfa-68b3-41c1-8ea3-36c8e7ace4bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(translate('buongiorno'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "happy new years\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PlIyinUD-Ch6",
        "colab_type": "code",
        "outputId": "c4f04175-2ebd-4646-d7e9-6effd8125ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "cell_type": "code",
      "source": [
        "text_it = None\n",
        "\n",
        "while text_it != \"ciao\":\n",
        "   \n",
        "  text_it = input(\"Italiano: \")\n",
        "  text_en = translate(text_it)\n",
        "\n",
        "  print('English:', text_en)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Italiano: come stai\n",
            "English: how is the sald\n",
            "\n",
            "Italiano: ciao\n",
            "English: he should thank me\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}